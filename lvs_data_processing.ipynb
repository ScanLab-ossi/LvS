{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import configparser\n",
    "import argparse \n",
    "from helpers import read\n",
    " \n",
    "import bottleneck as bn\n",
    "from LPA import Corpus, sockpuppet_distance\n",
    "from math import floor\n",
    "from scipy.spatial.distance import cdist, cityblock\n",
    "import matplotlib.pyplot as plt\n",
    "from visualize import sockpuppet_matrix, timeline\n",
    "alt.data_transformers.disable_max_rows()\n",
    "from unpivot_utils import unpivot_wide_dataframe\n",
    "from unpivot_utils import unpivot_dataframe \n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path,columns_to_remove):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data successfully loaded from {file_path}\")\n",
    "        print(f\"DataFrame shape: {df.shape}\") \n",
    "        #print 5 rows of the DataFrame\n",
    "        # remove the columns 'E1', 'E2', 'E3', 'E4' from the dataframe  \n",
    "        #columns_to_remove = ['E1', 'E2', 'E3', 'E4']\n",
    "        for col in columns_to_remove:\n",
    "            if col in df.columns:\n",
    "                print(f\"Removing column '{col}' from the DataFrame\")\n",
    "                df = df.drop(columns=[col])\n",
    "        # remove the column 'E5'  from the dataframe    \n",
    "        if 'E5' in df.columns:\n",
    "            print(f\"Removing column 'E5' from the DataFrame\")\n",
    "            df = df.drop(columns=['E5']) \n",
    "        print(df.head(5))\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None  # Important: Return None on error\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "def unpivot_data(df, agg_column , var_name , value_name ,ignore_columns, processing_type,file_path2):  \n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping unpivot_data.\")\n",
    "        return None \n",
    "    try:\n",
    "        if processing_type == 'full':\n",
    "            df_melted=unpivot_dataframe(df, ignore_columns, agg_column, var_name, value_name, file_path2)   \n",
    "\n",
    "             \n",
    "        elif processing_type == 'wide_unpivot': \n",
    "            ID_COLUMNS = ['element']\n",
    "\n",
    "            #unpivot_wide_csv_by_row('/Users/hagitbenshoshan/Documents/PHD/LVS_Code/LVS/data/alon/Allondata.csv',melted_df, ID_COLUMNS)\n",
    "            df_melted=unpivot_wide_dataframe(df,ID_COLUMNS,value_name='frequency_in_document', var_name='document')\n",
    "            # --- 4. Display the Results ---\n",
    "            print(\"\\n--- Resulting Long DataFrame ---\")\n",
    "            print(\"Shape:\", df_melted.shape)\n",
    "            print(\"First 10 rows:\")\n",
    "            print(df_melted.head(10))\n",
    "            \n",
    "        else:  \n",
    "            df_melted=df \n",
    "            df_melted = df_melted.reset_index() \n",
    "\n",
    "            \n",
    "        print(f\"agg_column: {agg_column}, var_name: {var_name}, value_name: {value_name}, ignore_columns: {ignore_columns}\")\n",
    "\n",
    "        df_melted_grouped = df_melted.groupby([agg_column, var_name])[value_name].sum().reset_index()\n",
    "\n",
    "        # Calculate the total deaths per year\n",
    "        df_melted_grouped['Total_Per_Agg'] = df_melted_grouped.groupby(agg_column)[value_name].transform('sum')\n",
    "        # Calculate the relative deaths\n",
    "        # additional column to calculate the relative [Optional]  \n",
    "\n",
    "        if file_path2 == 'None':\n",
    "                            \n",
    "            df_melted_grouped['frequency_in_document'] = df_melted_grouped[value_name] / df_melted_grouped['Total_Per_Agg']\n",
    "        else:   \n",
    "            \n",
    "            df2 = pd.read_csv(file_path2)   \n",
    "            # Merge with the population data\n",
    "            df_melted_grouped  = pd.merge(df_melted_grouped, df2, left_on=agg_column, right_on='Year', how='inner') \n",
    "            df_melted_grouped['frequency_in_document'] = df_melted_grouped[value_name] / df_melted_grouped['Population']\n",
    "\n",
    "        # Rename\n",
    "        df_melted_grouped = df_melted_grouped.rename(columns={agg_column:'document',\n",
    "                                var_name: 'element'}) \n",
    "        print(f\"Unpivoted data shape: {df_melted_grouped.head(5)}\")\n",
    "        return df_melted_grouped  \n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column not found: {e}.  Check your 'agg_column', 'var_name', 'value_name', and 'ignore_columns' parameters.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during unpivoting: {e}\")\n",
    "        return None\n",
    "def clean_data(df,columns_to_keep, short_names, dataset):\n",
    "    \"\"\"Cleans the DataFrame (e.g., keep only relevant columns , handles missing values, data type conversions).\"\"\"\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping clean_data.\")\n",
    "        return None, None\n",
    "    try:\n",
    "        #clean elements that always = 0 in all the documents \n",
    "        print(\"Cleaning data...\")\n",
    "        # Group by 'element' and sum the frequency across all documents\n",
    "        non_zero_elements = df.groupby('element')['frequency_in_document'].sum()\n",
    "        #print (f\"Non-zero elements: {non_zero_elements}\")\n",
    "        #print zero elements\n",
    "        #print (f\"Zero elements: {non_zero_elements[non_zero_elements == 0]}\")\n",
    "        # save the zero elements to a file\n",
    "        non_zero_elements[non_zero_elements == 0].to_csv(f\"results/{dataset}/zero_elements.csv\")\n",
    "        #save the non zero elements to a file\n",
    "        non_zero_elements[non_zero_elements > 0].to_csv(f\"results/{dataset}/non_zero_elements.csv\")\n",
    "        # Keep only elements with a non-zero total frequency\n",
    "        non_zero_elements = non_zero_elements[non_zero_elements > 0].index\n",
    "        # Filter the original DataFrame to keep only those elements\n",
    "        filtered_df = df[df['element'].isin(non_zero_elements)]\n",
    "        df= filtered_df\n",
    "        #print(f\"Filtered df 50: {df.head(50)}\")\n",
    "        df_cleaned = df.dropna()\n",
    "        # Keep only the relevant columns\n",
    "        df_cleaned = df_cleaned[columns_to_keep] \n",
    "        entity_code_df = None\n",
    "            # Shorten the element names\n",
    "        unique_elements = df['element'].unique()\n",
    "        #print(f\"Unique elements: {unique_elements}\")\n",
    "        if short_names=='True':\n",
    "            element_to_code = { element: f'E{i}' for i,  element  in enumerate(unique_elements) }\n",
    "        else:\n",
    "            # Create a mapping from element names to codes\n",
    "            element_to_code = {element: element for i, element in enumerate(unique_elements)}\n",
    "            \n",
    "        df_cleaned['element'] = df_cleaned['element'].map(element_to_code)  \n",
    "        # Create a DataFrame from the dictionary\n",
    "        entity_code_df = pd.DataFrame(list(element_to_code.items()), columns=['element_name', 'element']) \n",
    "        #df_cleaned['amount'] = pd.to_numeric(df_cleaned['amount'], errors='coerce') \n",
    "        return df_cleaned, entity_code_df\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column not found: {e}. Check your 'columns_to_keep' parameter.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data cleaning: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "def filter_data(df, column, condition):\n",
    "    \"\"\"Filters the DataFrame based on a condition.\"\"\"\n",
    "    print(f\"Filtering data where {column} {condition}\")\n",
    "    return df[df[column] > condition]\n",
    "\n",
    "def calculate_summary(df, group_by_column, aggregation):\n",
    "    \"\"\"Calculates summary statistics on the DataFrame.\"\"\"\n",
    "    print(f\"Calculating summary by {group_by_column}...\")\n",
    "    return df.groupby(group_by_column).agg(aggregation)\n",
    "\n",
    "def save_results(df,entity_code_df, output_path,output_dic):\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping save_results.\")\n",
    "        return\n",
    "\n",
    "    \"\"\"Saves the processed DataFrames to a CSV file.\"\"\"\n",
    "    print(f\"Saving results to: {output_path} and {output_dic}\") \n",
    "    try: \n",
    "        print(f\"Saving results to: {output_path} and {output_dic}\")\n",
    "        df.to_csv(output_path, index=False)  # Don't include the index\n",
    "        print(f\"Data successfully saved to {output_path}\")\n",
    "\n",
    "        if output_dic:\n",
    "            #  Create a DataFrame from the dictionary and save it.  Important for consistent structure.\n",
    "            entity_code_df.to_csv(output_dic, index=False)            \n",
    "            print(f\"Dictionary successfully saved to {output_path.replace('.csv', '_dict.csv')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_signatures(df, entity_code_df, sig_file, dataset,graph,top,sig_length,var_name,value_name):\n",
    "    \"\"\"\n",
    "    Generates and saves document signatures, along with related analyses and visualizations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing document data.\n",
    "        entity_code_df (pd.DataFrame, optional): DataFrame mapping entity codes to names.\n",
    "        sig_file (str, optional): Path to save the signature DataFrame.\n",
    "        dataset (str): Name of the dataset for output directory.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping generate_signatures.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create results directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(f\"results/{dataset}\", exist_ok=True)\n",
    "        print (f\"sig_file: {sig_file}, dataset: {dataset}, graph: {graph}, top: {top}, sig_length: {sig_length}, var_name: {var_name}, value_name: {value_name}\")\n",
    "\n",
    "        corpus = Corpus(df, \"document\", \"element\", \"frequency_in_document\")\n",
    "        dvr = corpus.create_dvr(equally_weighted=True) # Create Document Vector Representation (DVR)\n",
    "        dvr.to_csv(f\"results/{dataset}/dvr.csv\")\n",
    "        top = int(top)\n",
    "        sig_length = int(sig_length)\n",
    "\n",
    "        sigs = corpus.create_signatures(distance=\"JSD\",sig_length=sig_length, most_significant=top,prevalent=0.1) #Hagit check if this is the right distance\n",
    "\n",
    "        #  Saving top N changed elements\n",
    "        sigs[1].to_csv(f\"results/{dataset}/top_{top}_most_changed.csv\")\n",
    "        sig = pd.DataFrame(sigs[1])\n",
    "\n",
    "        # Rename columns based on entity_code_df if provided\n",
    "        if entity_code_df is not None:\n",
    "            entity_code_to_name = entity_code_df.set_index(\"element\")[\"element_name\"].to_dict()\n",
    "            new_columns = [\n",
    "                entity_code_to_name.get(col, col) for col in sig.columns\n",
    "            ]  # Use get() for safety\n",
    "            sig.columns = new_columns\n",
    "            sig.to_csv(f\"results/{dataset}/top_{top}_most_changed_real_names.csv\")\n",
    "\n",
    "        # Save signatures if sig_file is provided\n",
    "        if sig_file:\n",
    "            ndf = pd.DataFrame(sigs[0])\n",
    "            ndf.to_csv(sig_file, index=True)\n",
    "            print(f\"Signatures successfully saved to {sig_file}\")\n",
    "            #save the signatures with real names\n",
    "            if entity_code_df is not None:\n",
    "                ndf.columns = [entity_code_to_name.get(col, col) for col in ndf.columns]\n",
    "                ndf.to_csv(sig_file.replace('.csv', '_real_names.csv'), index=True)\n",
    "                print(f\"Signatures with real names successfully saved to {sig_file.replace('.csv', '_real_names.csv')}\")\n",
    "        else:\n",
    "            print(\"No signature file provided, skipping signature saving.\")\n",
    "\n",
    "        # split the ndf DataFrame  to several dataframes   , by the column name   \n",
    "        output_dir = f\"results/{dataset}/split_dataframes\"\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Iterate over each row of the DataFrame using iterrows()\n",
    "        # This method yields both the index and the row (as a Series)\n",
    "        for index, row in ndf.iterrows():\n",
    "            # Convert the row (which is a pandas Series) to a DataFrame\n",
    "            # .to_frame() converts the Series to a DataFrame with the original Series index as the new DataFrame's index\n",
    "            # We can provide a column name, for instance, using the original index\n",
    "            \n",
    "            row = row[row.notnull()]\n",
    "\n",
    "            row_df = row.to_frame(name=f'row_{index}_data')\n",
    "\n",
    "            # Pivot the result. For a single row DataFrame, transposing it achieves the desired pivoted effect.\n",
    "            pivoted_df = row_df \n",
    "\n",
    "\n",
    "            # Define a unique filename for each new CSV file\n",
    "            file_name = os.path.join(output_dir, f'row_{index}.csv') \n",
    "\n",
    "            # If the DataFrame is empty after dropping NaN columns, skip saving\n",
    "            if pivoted_df.empty:    \n",
    "                print(f\"Row {index} has no data to save, skipping.\")\n",
    "                continue    \n",
    "\n",
    "            # Save the pivoted DataFrame to a new CSV file.\n",
    "            # The column names from the original DataFrame will be preserved as the header.\n",
    "            # Select rows where the second column (index 1) is not null\n",
    "\n",
    "            pivoted_df = pivoted_df[pivoted_df.iloc[:, 0].notna()]\n",
    "            pivoted_sorted_desc = pivoted_df.sort_values(by=pivoted_df.columns[0], ascending=False)\n",
    "            #pivoted_sorted_desc['col1_numeric'] = pd.to_numeric(pivoted_sorted_desc[pivoted_sorted_desc.columns[0]], errors='coerce')\n",
    "            #pivoted_sorted_desc= pivoted_sorted_desc.sort_values(by=pivoted_sorted_desc.columns[1].abs(), ascending=False)\n",
    "            pivoted_sorted_desc.to_csv(file_name, index=True)\n",
    "            \n",
    "            # Convert column[0] to numeric. 'coerce' will turn invalid parsing into NaN.\n",
    "            ''' \n",
    "            pivoted_df['col1_numeric'] = pd.to_numeric(pivoted_df[pivoted_df.columns[0]], errors='coerce')\n",
    "\n",
    "            # Drop rows where conversion failed (optional, depending on how you want to handle errors)\n",
    "            pivoted_df.dropna(subset=['col1_numeric'], inplace=True)\n",
    "\n",
    "            # Sort rx_filtered by the absolute value of the newly created numeric column in descending order\n",
    "            rx_sorted_abs_desc = pivoted_df.sort_values(by=pivoted_df['col1_numeric'].abs(), ascending=False)\n",
    "\n",
    "            # Drop the temporary numeric column if you don't need it\n",
    "            rx_sorted_abs_desc = rx_sorted_abs_desc.drop(columns=['col1_numeric'])\n",
    "            rx_sorted_abs_desc.to_csv(file_name, index=True)\n",
    "            '''\n",
    "\n",
    "            print(f\"Saved pivoted data for row {index} to '{file_name}'\") \n",
    "\n",
    "           # Save element list\n",
    "        with open(f\"results/{dataset}/list.txt\", \"w\") as f:\n",
    "            for item in sigs[0]:\n",
    "                f.write(f\"{item}\\n\")\n",
    "        # save list into dataframe  \n",
    "        df_list = pd.DataFrame(sigs[0])\n",
    "        print(f\"Element list saved to results/{dataset}/list.txt\")\n",
    "        #print(df_list.head(5))\n",
    "        #pivot_the_list = df_list.melt(var_name='element', value_name='value', ignore_index=False)\n",
    "        print(df_list.columns)\n",
    "        #print(df_list.head(5))\n",
    "        pivot_the_list = df_list.melt(var_name=var_name, value_name=value_name, ignore_index=False)\n",
    "        pivot_the_list = pivot_the_list.reset_index().rename(columns={'index': 'document'})\n",
    "        df_list = pivot_the_list.dropna().reset_index(drop=True)   \n",
    "        df_list.to_csv(f\"results/{dataset}/list.csv\", index=True)\n",
    "        print(f\"Element list saved to results/{dataset}/list.csv\")      \n",
    "\n",
    "        # Sockpuppet analysis\n",
    "        if graph == 'True':\n",
    "            ecorpus = Corpus(df)\n",
    "            ecorpus_dvr = ecorpus.create_dvr(equally_weighted=True)  # Corrected variable name\n",
    "            esigs = ecorpus.create_signatures(distance=\"JSD\")\n",
    "            espd = sockpuppet_distance(ecorpus, ecorpus, heuristic=False, distance=\"euclidean\")\n",
    "            chart = sockpuppet_matrix(espd)\n",
    "            if chart is not None:\n",
    "                try:\n",
    "                    chart.save(f\"results/{dataset}/sockpuppet_distance_matrix.png\", scale_factor=4.0)\n",
    "                    print(f\"Sockpuppet distance matrix chart saved to results/{dataset}/sockpuppet_distance_matrix.png\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving sockpuppet distance matrix chart: {e}\")\n",
    "\n",
    "            espd.to_csv(f\"results/{dataset}/sockpuppet_distance_matrix.csv\", index=False)\n",
    "\n",
    "            # Top 10 distances chart\n",
    "            try:\n",
    "                top_changing = sig[sig.sum(0).abs().sort_values(ascending=False).head(10).index]\n",
    "                chart = (\n",
    "                    alt.Chart(\n",
    "                        top_changing.reset_index()\n",
    "                        .melt(id_vars=\"index\")\n",
    "                        .rename(\n",
    "                            columns={\n",
    "                                \"index\": \"Year\",\n",
    "                                \"variable\": \"Element\",\n",
    "                                \"value\": \"Distance from PM\",\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    .mark_line()\n",
    "                    .encode(x=\"Year:N\", y=\"Distance from PM\", color=\"Element\")\n",
    "                    .properties(width=300, height=300, title=\"\")\n",
    "                )\n",
    "                chart.save(f\"results/{dataset}/top_10_distances.png\", scale_factor=4.0)\n",
    "                print(f\"Top 10 distances chart saved to results/{dataset}/top_10_distances.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating or saving top 10 distances chart: {e}\")\n",
    "\n",
    "            try:\n",
    "                # PCA analysis \n",
    "                # Convert long-form distance data to square symmetric matrix\n",
    "                espd_matrix = espd.pivot(index=\"Corpus 1\", columns=\"Corpus 2\", values=\"value\")\n",
    "\n",
    "                # Fill missing values, enforce symmetry, and zero diagonal\n",
    "                espd_matrix = espd_matrix.fillna(0)\n",
    "                espd_matrix = espd_matrix + espd_matrix.T\n",
    "                np.fill_diagonal(espd_matrix.values, 0)\n",
    "\n",
    "                # === PCA computation ===\n",
    "                pca = PCA(n_components=2)\n",
    "                pca_result = pca.fit_transform(espd_matrix)\n",
    "                explained_std = pca.explained_variance_ratio_\n",
    "\n",
    "                pca_df = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
    "                pca_df[\"alias\"] = espd_matrix.index\n",
    "                 # === Save raw PCA plot ===\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], alpha=0.7)\n",
    "                for i, label in enumerate(pca_df[\"alias\"]):\n",
    "                    plt.text(pca_df.loc[i, \"PC1\"], pca_df.loc[i, \"PC2\"], label, fontsize=6, alpha=0.7)\n",
    "                plt.xlabel(f\"PC1 ({explained_std[0]:.2%} variance)\")\n",
    "                plt.ylabel(f\"PC2 ({explained_std[1]:.2%} variance)\")\n",
    "                plt.title(\"Raw PCA (Sockpuppet Distance Matrix)\")\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"results/{dataset}/pca_raw.png\", dpi=300)\n",
    "                plt.close()\n",
    "                print(f\"Raw PCA saved to results/{dataset}/pca_raw.png\")\n",
    "                merged_df=pca_df\n",
    "\n",
    " \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating or saving PCA chart: {e}\")\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failure in generate_signatures: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline  \n",
    "# Reuse the functions from the basic example\n",
    "# clean_data, filter_data, calculate_summary, save_results\n",
    "\n",
    "def process_data(file_path, file_path2, ignore_columns, columns_to_keep,columns_to_remove,agg_column, var_name, value_name, output_path, output_dic, processing_type, sig_file,dataset,graph,top,sig_length,short_names):\n",
    "    \"\"\"\n",
    "    Pipeline function to load, unpivot, clean, and save data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        file_path2 (str): Path to the second input CSV file.\n",
    "        ignore_columns (list): List of columns to ignore during unpivoting.\n",
    "        columns_to_keep (dict): Columns to keep and their new names.\n",
    "        agg_column (str): Column to aggregate by during unpivoting.\n",
    "        var_name (str): Name for the variable column after unpivoting.\n",
    "        value_name (str): Name for the value column after unpivoting.\n",
    "        output_path (str): Path to save the processed CSV file.\n",
    "        output_dic (dict, optional): Dictionary to save as a CSV file.\n",
    "    \"\"\"\n",
    "    df = load_data(file_path,columns_to_remove)\n",
    "    if df is None:\n",
    "        print(\"Pipeline aborted due to error in load_data.\")\n",
    "        return  # Stop the pipeline\n",
    "\n",
    "    df_unpivoted = unpivot_data(df, agg_column, var_name, value_name, ignore_columns,processing_type, file_path2)\n",
    "    if df_unpivoted is None:\n",
    "        print(\"Pipeline aborted due to error in unpivot_data.\")\n",
    "        return\n",
    "\n",
    "    df_cleaned ,entity_code_df = clean_data(df_unpivoted, columns_to_keep,short_names, dataset)\n",
    "    # print(df_cleaned  ) \n",
    "    if df_cleaned is None:\n",
    "        print(\"Pipeline aborted due to error in clean_data.\")  \n",
    "        return\n",
    " \n",
    "    save_results(df_cleaned,entity_code_df, output_path, output_dic)\n",
    "    print(\"Pipeline execution complete!\")\n",
    "\n",
    "    print (\"Generating signatures...\")\n",
    "    print (f\"sig_file: {sig_file}, dataset: {dataset}, graph: {graph}, top: {top}, sig_length: {sig_length}, var_name: {var_name}, value_name: {value_name}\")\n",
    "    generate_signatures(df_cleaned,entity_code_df,sig_file,dataset,graph,top,sig_length,var_name,value_name)  \n",
    "    print(\"signatures execution complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to remove: ['E5', 'E6', 'E8']\n",
      "Data successfully loaded from data/alon/Allondata.csv\n",
      "DataFrame shape: (20465, 49)\n",
      "Removing column 'E5' from the DataFrame\n",
      "Removing column 'E6' from the DataFrame\n",
      "Removing column 'E8' from the DataFrame\n",
      "         element          F8          D8          E7          A1          E2  \\\n",
      "0  0610007P14RIK  185.893620  182.452130  143.864400  158.923900  154.095500   \n",
      "1  0610009B22RIK   30.173896   32.088989   25.374459   30.583817   26.888653   \n",
      "2  0610009L18RIK    0.548329    0.909437    0.711346    0.151137    0.985998   \n",
      "3  0610009O20RIK   20.541800   19.744800   22.925100   25.637200   22.331200   \n",
      "4  0610010F05RIK    6.384258    4.299950    5.403890    4.810547    5.912972   \n",
      "\n",
      "           F2          D2          B5          C5  ...          B1  \\\n",
      "0  173.223200  185.355900  174.255800  191.887100  ...  173.328100   \n",
      "1   33.595935   48.871509   34.083420   43.931272  ...   32.220310   \n",
      "2    0.000000    1.572330    0.195557    0.636702  ...    0.315226   \n",
      "3   22.958500   26.592900   26.009400   24.039700  ...   22.153400   \n",
      "4    9.561192    6.578846    7.917890    7.330905  ...    6.502800   \n",
      "\n",
      "           C1          F6          F3          A4          D3          B4  \\\n",
      "0  199.462600  154.974500  198.764100  204.322000  188.100700  189.928500   \n",
      "1   49.969670   27.406650   29.180470   31.348629   55.518641   34.537870   \n",
      "2    1.175520    0.576902    0.206386    2.092080    2.178090    0.621090   \n",
      "3   28.415000   26.161300   19.767800   27.716500   22.796100   20.845700   \n",
      "4    6.364321    7.092618    9.858441    5.799990    5.529577    8.614883   \n",
      "\n",
      "           C4          A5          E3  \n",
      "0  194.437700  174.412300  176.015700  \n",
      "1   48.071623   35.692140   32.249770  \n",
      "2    2.332440    0.924304    0.652935  \n",
      "3   29.103500   23.497300   28.271600  \n",
      "4    6.785088    4.349914    5.652854  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "Unpivoting the DataFrame...\n",
      "Unpivot process complete.\n",
      "\n",
      "--- Resulting Long DataFrame ---\n",
      "Shape: (920925, 3)\n",
      "First 10 rows:\n",
      "         element document  frequency_in_document\n",
      "0  0610007P14RIK       F8             185.893620\n",
      "1  0610009B22RIK       F8              30.173896\n",
      "2  0610009L18RIK       F8               0.548329\n",
      "3  0610009O20RIK       F8              20.541800\n",
      "4  0610010F05RIK       F8               6.384258\n",
      "5  0610010K14RIK       F8              84.214630\n",
      "6  0610011F06RIK       F8              79.640400\n",
      "7  0610030E20RIK       F8               7.991720\n",
      "8  0610037L13RIK       F8              30.631150\n",
      "9  0610040J01RIK       F8               0.090851\n",
      "agg_column: document, var_name: element, value_name: frequency_in_document, ignore_columns: ['Entity', 'Code']\n",
      "Unpivoted data shape:   document        element  frequency_in_document  Total_Per_Agg\n",
      "0       A1  0610007P14RIK           1.779556e-04  893053.810988\n",
      "1       A1  0610009B22RIK           3.424633e-05  893053.810988\n",
      "2       A1  0610009L18RIK           1.692362e-07  893053.810988\n",
      "3       A1  0610009O20RIK           2.870734e-05  893053.810988\n",
      "4       A1  0610010F05RIK           5.386626e-06  893053.810988\n",
      "Cleaning data...\n",
      "Saving results to: results/alon/lvs.csv and results/alon/dic.csv\n",
      "Saving results to: results/alon/lvs.csv and results/alon/dic.csv\n",
      "Data successfully saved to results/alon/lvs.csv\n",
      "Dictionary successfully saved to results/alon/lvs_dict.csv\n",
      "Pipeline execution complete!\n",
      "Generating signatures...\n",
      "sig_file: results/alon/signatures.csv, dataset: alon, graph: True, top: 25, sig_length: 200, var_name: element, value_name: frequency_in_document\n",
      "sig_file: results/alon/signatures.csv, dataset: alon, graph: True, top: 25, sig_length: 200, var_name: element, value_name: frequency_in_document\n",
      "       element_code  element  global_weight\n",
      "0             12851     PPIA   1.136524e-02\n",
      "1             14133    RPL41   9.112226e-03\n",
      "2              4807   EEF1A1   8.066683e-03\n",
      "3             14142    RPLP1   7.706360e-03\n",
      "4             14141    RPLP0   6.542907e-03\n",
      "...             ...      ...            ...\n",
      "19047          3502  COL13A1   2.723753e-17\n",
      "19048          6462  GM14632   2.513081e-17\n",
      "19049          6296  GM10096   2.513081e-17\n",
      "19050          6573   GM2863   1.954278e-17\n",
      "19051         18008  VMN2R11   1.812898e-17\n",
      "\n",
      "[19052 rows x 3 columns]\n",
      "\n",
      "\n",
      "Signatures successfully saved to results/alon/signatures.csv\n",
      "Signatures with real names successfully saved to results/alon/signatures_real_names.csv\n",
      "------------------------------\n",
      "Saved pivoted data for row A1 to 'results/alon/split_dataframes/row_A1.csv'\n",
      "Saved pivoted data for row A2 to 'results/alon/split_dataframes/row_A2.csv'\n",
      "Saved pivoted data for row A3 to 'results/alon/split_dataframes/row_A3.csv'\n",
      "Saved pivoted data for row A4 to 'results/alon/split_dataframes/row_A4.csv'\n",
      "Saved pivoted data for row A5 to 'results/alon/split_dataframes/row_A5.csv'\n",
      "Saved pivoted data for row A6 to 'results/alon/split_dataframes/row_A6.csv'\n",
      "Saved pivoted data for row A7 to 'results/alon/split_dataframes/row_A7.csv'\n",
      "Saved pivoted data for row A8 to 'results/alon/split_dataframes/row_A8.csv'\n",
      "Saved pivoted data for row B1 to 'results/alon/split_dataframes/row_B1.csv'\n",
      "Saved pivoted data for row B2 to 'results/alon/split_dataframes/row_B2.csv'\n",
      "Saved pivoted data for row B3 to 'results/alon/split_dataframes/row_B3.csv'\n",
      "Saved pivoted data for row B4 to 'results/alon/split_dataframes/row_B4.csv'\n",
      "Saved pivoted data for row B5 to 'results/alon/split_dataframes/row_B5.csv'\n",
      "Saved pivoted data for row B6 to 'results/alon/split_dataframes/row_B6.csv'\n",
      "Saved pivoted data for row B7 to 'results/alon/split_dataframes/row_B7.csv'\n",
      "Saved pivoted data for row B8 to 'results/alon/split_dataframes/row_B8.csv'\n",
      "Saved pivoted data for row C1 to 'results/alon/split_dataframes/row_C1.csv'\n",
      "Saved pivoted data for row C2 to 'results/alon/split_dataframes/row_C2.csv'\n",
      "Saved pivoted data for row C3 to 'results/alon/split_dataframes/row_C3.csv'\n",
      "Saved pivoted data for row C4 to 'results/alon/split_dataframes/row_C4.csv'\n",
      "Saved pivoted data for row C5 to 'results/alon/split_dataframes/row_C5.csv'\n",
      "Saved pivoted data for row C6 to 'results/alon/split_dataframes/row_C6.csv'\n",
      "Saved pivoted data for row C7 to 'results/alon/split_dataframes/row_C7.csv'\n",
      "Saved pivoted data for row C8 to 'results/alon/split_dataframes/row_C8.csv'\n",
      "Saved pivoted data for row D1 to 'results/alon/split_dataframes/row_D1.csv'\n",
      "Saved pivoted data for row D2 to 'results/alon/split_dataframes/row_D2.csv'\n",
      "Saved pivoted data for row D3 to 'results/alon/split_dataframes/row_D3.csv'\n",
      "Saved pivoted data for row D4 to 'results/alon/split_dataframes/row_D4.csv'\n",
      "Saved pivoted data for row D5 to 'results/alon/split_dataframes/row_D5.csv'\n",
      "Saved pivoted data for row D6 to 'results/alon/split_dataframes/row_D6.csv'\n",
      "Saved pivoted data for row D7 to 'results/alon/split_dataframes/row_D7.csv'\n",
      "Saved pivoted data for row D8 to 'results/alon/split_dataframes/row_D8.csv'\n",
      "Saved pivoted data for row E1 to 'results/alon/split_dataframes/row_E1.csv'\n",
      "Saved pivoted data for row E2 to 'results/alon/split_dataframes/row_E2.csv'\n",
      "Saved pivoted data for row E3 to 'results/alon/split_dataframes/row_E3.csv'\n",
      "Saved pivoted data for row E4 to 'results/alon/split_dataframes/row_E4.csv'\n",
      "Saved pivoted data for row E7 to 'results/alon/split_dataframes/row_E7.csv'\n",
      "Saved pivoted data for row F1 to 'results/alon/split_dataframes/row_F1.csv'\n",
      "Saved pivoted data for row F2 to 'results/alon/split_dataframes/row_F2.csv'\n",
      "Saved pivoted data for row F3 to 'results/alon/split_dataframes/row_F3.csv'\n",
      "Saved pivoted data for row F4 to 'results/alon/split_dataframes/row_F4.csv'\n",
      "Saved pivoted data for row F5 to 'results/alon/split_dataframes/row_F5.csv'\n",
      "Saved pivoted data for row F6 to 'results/alon/split_dataframes/row_F6.csv'\n",
      "Saved pivoted data for row F7 to 'results/alon/split_dataframes/row_F7.csv'\n",
      "Saved pivoted data for row F8 to 'results/alon/split_dataframes/row_F8.csv'\n",
      "Element list saved to results/alon/list.txt\n",
      "Index(['RPS2', 'IL9', 'LTA', 'LSM4', 'SRM', 'EIF3F', 'AQP3', 'RNF187', 'VIM',\n",
      "       'DUSP2',\n",
      "       ...\n",
      "       'SSR2', 'FURIN', 'PRDX6', 'CTSC', 'IL4RA', 'CYP1B1', 'CASP3', 'STX11',\n",
      "       'GALK2', 'SLC43A3'],\n",
      "      dtype='object', length=921)\n",
      "Element list saved to results/alon/list.csv\n",
      "       element_code  element  global_weight\n",
      "0             12851     PPIA   1.136524e-02\n",
      "1             14133    RPL41   9.112226e-03\n",
      "2              4807   EEF1A1   8.066683e-03\n",
      "3             14142    RPLP1   7.706360e-03\n",
      "4             14141    RPLP0   6.542907e-03\n",
      "...             ...      ...            ...\n",
      "19047          3502  COL13A1   2.723753e-17\n",
      "19048          6462  GM14632   2.513081e-17\n",
      "19049          6296  GM10096   2.513081e-17\n",
      "19050          6573   GM2863   1.954278e-17\n",
      "19051         18008  VMN2R11   1.812898e-17\n",
      "\n",
      "[19052 rows x 3 columns]\n",
      "\n",
      "\n",
      "Sockpuppet distance matrix chart saved to results/alon/sockpuppet_distance_matrix.png\n",
      "Top 10 distances chart saved to results/alon/top_10_distances.png\n",
      "Raw PCA saved to results/alon/pca_raw.png\n",
      "signatures execution complete!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. Set up argument parser\n",
    "    #parser = argparse.ArgumentParser(description=\"Process data from a CSV file.\")\n",
    "    #parser.add_argument(\"--config\", help=\"Path to the config file\", default=\"config.toml\")\n",
    "    #args = parser.parse_args()\n",
    "    config_file_path = 'config_alon.toml'  # Replace with your actual path\n",
    "\n",
    "    # 2. Read the config file\n",
    "    config = configparser.ConfigParser()\n",
    "    #config.read(args.config)\n",
    "    config.read(config_file_path)\n",
    "    # 3. Get parameters from the config\n",
    "    file_path = config.get(\"data\", \"file_path\")\n",
    "    file_path2 = config.get(\"data\", \"file_path2\")    \n",
    "    agg_column=config.get(\"proc\",\"agg_column\")\n",
    "    var_name=config.get(\"proc\",\"var_name\") \n",
    "    value_name=config.get(\"proc\",\"value_name\")  \n",
    "    processing_type = config.get(\"proc\",\"processing_type\")\n",
    "    columns_to_remove = config.get(\"proc\", \"columns_to_remove\").split(',') if config.has_option(\"proc\", \"columns_to_remove\") else []\n",
    "    # Convert to list if it's a comma-separated string\n",
    "    columns_to_remove = [col.strip() for col in columns_to_remove if col.strip()]  # Remove empty strings\n",
    "    # If the config file has no columns to remove, it will be an empty list \n",
    "    if not columns_to_remove:\n",
    "        print(\"No columns to remove specified in the config file.\")\n",
    "    else:\n",
    "        print(f\"Columns to remove: {columns_to_remove}\")    \n",
    "    #   \n",
    "    output_path = config.get(\"output\", \"output_path\") \n",
    "    output_dic = config.get(\"output\", \"output_dic\")  \n",
    "    sig_file = config.get(\"output\", \"sig_file\") \n",
    "    dataset = config.get(\"data\", \"dataset\")\n",
    "    graph = config.get(\"output\", \"graph\")\n",
    "    top = config.get(\"output\", \"top\")\n",
    "    sig_length = config.get(\"output\", \"sig_length\")\n",
    "    short_names  = config.get(\"output\", \"short_names\")\n",
    "    # constants \n",
    "    ignore_columns = ['Entity','Code']\n",
    "    columns_to_keep = ['document', 'element', 'frequency_in_document']  \n",
    "    #ignore_columns = []\n",
    "    # 4. Call the processing function\n",
    "    process_data(file_path,file_path2,ignore_columns,columns_to_keep,columns_to_remove,agg_column,var_name,value_name,output_path,output_dic,processing_type,sig_file,dataset,graph,top,sig_length,short_names) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
